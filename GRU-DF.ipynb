{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","!conda install -file env.yaml"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5J04fUqIvFSS"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ty/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n","  return torch._C._cuda_getDeviceCount() > 0\n","/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/grud.py:114: RuntimeWarning: Mean of empty slice\n","  empirical_mean = np.nanmean(X, axis=1)\n","/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/grudf.py:122: RuntimeWarning: Mean of empty slice\n","  empirical_mean_1 = np.nanmean(X, axis=1)\n","  0%|          | 1/1000 [00:40<11:19:17, 40.80s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/1000, Training Loss: 0.0338, Validation Loss: 0.0224\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1000 [01:21<11:15:42, 40.62s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/1000, Training Loss: 0.0177, Validation Loss: 0.0137\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 3/1000 [02:00<11:07:26, 40.17s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/1000, Training Loss: 0.0124, Validation Loss: 0.0106\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 3/1000 [02:35<14:19:01, 51.70s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-DF.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-DF.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m X_batch \u001b[39m=\u001b[39m new_batch[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-DF.ipynb#W1sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m y_batch \u001b[39m=\u001b[39m new_batch[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-DF.ipynb#W1sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49minput_batch)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-DF.ipynb#W1sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(X_batch, predictions, y_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-DF.ipynb#W1sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n","File \u001b[0;32m~/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/Gatech/Fa2023/Deep_Learning_Final_Project/grudf.py:95\u001b[0m, in \u001b[0;36mGRU_DF.forward\u001b[0;34m(self, X, delta, delta_future, M, last_observation, next_observation, empirical_mean)\u001b[0m\n\u001b[1;32m     93\u001b[0m h_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(X\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru_d_cell\u001b[39m.\u001b[39mhidden_size, device\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(X\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)):\n\u001b[0;32m---> 95\u001b[0m     h_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru_d_cell(X[:, t, :], delta[:, t, :], delta_future[:, t, :], M[:, t, :], h_t, last_observation[:, t, :], next_observation[:, t, :], \n\u001b[1;32m     96\u001b[0m                           empirical_mean)\n\u001b[1;32m     97\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(h_t))\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39msqueeze()\n","File \u001b[0;32m~/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/Gatech/Fa2023/Deep_Learning_Final_Project/grudf.py:70\u001b[0m, in \u001b[0;36mGRU_DF_Cell.forward\u001b[0;34m(self, x, delta, delta_future, m, h_prev, x_last_observed, x_next_observed, empirical_mean)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, delta, delta_future, m, h_prev, x_last_observed, x_next_observed, empirical_mean):\n\u001b[1;32m     69\u001b[0m     gamma_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39mzeros_like(delta), torch\u001b[39m.\u001b[39mmatmul(delta, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay_Wx\u001b[39m.\u001b[39mt()) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay_bx))\n\u001b[0;32m---> 70\u001b[0m     gamma_h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mexp(\u001b[39m-\u001b[39;49mtorch\u001b[39m.\u001b[39;49mmax(torch\u001b[39m.\u001b[39;49mzeros_like(h_prev), torch\u001b[39m.\u001b[39;49mmatmul(delta, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecay_Wh\u001b[39m.\u001b[39;49mt()) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecay_bh))\n\u001b[1;32m     71\u001b[0m     gamma_x_future \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39mzeros_like(delta_future), \n\u001b[1;32m     72\u001b[0m                                           torch\u001b[39m.\u001b[39mmatmul(delta_future, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay_future_Wx\u001b[39m.\u001b[39mt()) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay_future_bx))\n\u001b[1;32m     74\u001b[0m     x_nan_to_num \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnan_to_num(x)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import numpy as np\n","from tqdm import tqdm\n","from datasets import GRUDFdataset\n","from grudf import GRU_DF, preprocess_dataset, GRUDF_Weighted_Loss\n","from accuracies import accuracies\n","\n","torch.manual_seed(0)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","X = np.load('data_X.npy')\n","y = np.load('data_y.npy')\n","\n","#Negative/positive weight calculated as whole before runtime\n","neg_weight = np.sum(y == 0)/y.size\n","pos_weight = np.sum(y == 1)/y.size\n","\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","train_index, test_index = next(sss.split(X, y))\n","val_index, test_index = next(StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0).split(X[test_index], y[test_index]))\n","\n","X, delta, delta_future, M, last_observation, next_observation, empirical_mean = preprocess_dataset(X)\n","\n","train_dataset = GRUDFdataset(X[train_index], delta[train_index], delta_future[train_index],\n","                             M[train_index], last_observation[train_index], next_observation[train_index], empirical_mean[train_index], y[train_index])\n","val_dataset = GRUDFdataset(X[val_index], delta[val_index], delta_future[val_index], \n","                           M[val_index], last_observation[val_index], next_observation[val_index], empirical_mean[val_index], y[val_index])\n","test_dataset = GRUDFdataset(X[test_index], delta[test_index], delta_future[test_index], \n","                            M[test_index], last_observation[test_index], next_observation[test_index], empirical_mean[test_index], y[test_index])\n","\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","input_size = 5\n","hidden_size = 16\n","output_size = 1\n","lr_rate = 0.001\n","\n","model = GRU_DF(input_size=input_size, hidden_size=hidden_size)\n","model.to(device)\n","\n","criterion = GRUDF_Weighted_Loss(pos_weight, neg_weight)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n","\n","early_stopping_patience = 20\n","early_stopping_counter = 0\n","best_val_loss = float('inf')\n","model_path = 'best_model_GRUDF.pth'\n","\n","num_epochs = 1000\n","for epoch in tqdm(range(num_epochs)):\n","    model.train()\n","    running_loss = 0.0\n","    for batch in train_loader:\n","        new_batch = []\n","        for item in batch:\n","            new_batch.append(item.to(device))\n","        input_batch = tuple(new_batch[:-1])\n","        X_batch = new_batch[0]\n","        y_batch = new_batch[-1]\n","        predictions = model(*input_batch).squeeze()\n","        loss = criterion(X_batch, predictions, y_batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * input_batch[0].size(0)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            new_batch = []\n","            for item in batch:\n","                new_batch.append(item.to(device))\n","            input_batch = tuple(new_batch[:-1])\n","            X_batch = new_batch[0]\n","            y_batch = new_batch[-1]\n","            predictions = model(*input_batch).squeeze()\n","            loss = criterion(X_batch, predictions, y_batch)\n","            val_loss += loss.item() * input_batch[0].size(0)\n","\n","    train_loss = running_loss / len(train_loader.dataset)\n","    val_loss = val_loss / len(val_loader.dataset)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    scheduler.step()\n","\n","    if epoch % 10 == 0:\n","        torch.save(model.state_dict(), f'best_model_GRUDF_{epoch}.pth')\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        early_stopping_counter = 0\n","        torch.save(model.state_dict(), model_path)\n","    else:\n","        early_stopping_counter += 1\n","        if early_stopping_counter >= early_stopping_patience:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","model.load_state_dict(torch.load(model_path))\n","\n","model.eval()\n","test_losses = []\n","test_labels = []\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for batch in val_loader:\n","        new_batch = []\n","        for item in batch:\n","            new_batch.append(item.to(device))\n","        input_batch = tuple(new_batch[:-1])\n","        y_batch = new_batch[-1:][0]\n","        outputs = model(*input_batch).squeeze()\n","        loss = criterion(outputs, y_batch)\n","        test_losses.append(loss.item())\n","        test_predictions.extend(outputs.tolist())\n","        test_labels.extend(y_batch.tolist())\n","\n","average_test_loss = sum(test_losses) / len(test_losses)\n","print(f'Average test loss: {average_test_loss:.4f}')\n","\n","test_predictions = np.array(test_predictions)\n","test_labels = np.array(test_labels)\n","\n","accuracies(test_labels, test_predictions)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"fqRIy9dBOplT"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average test loss: 0.3622\n","AUC score: 0.8319\n","Accuracy: 0.7537\n","Sensitivity: 0.7987\n","Specificity: 0.7438\n"]}],"source":["model.load_state_dict(torch.load(model_path))\n","\n","model.eval()\n","test_losses = []\n","test_labels = []\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch in test_loader:\n","        X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch \\\n","         = X_batch.to(device), delta_batch.to(device), M_batch.to(device), last_observation_batch.to(device), empirical_mean_batch.to(device), y_batch.to(device)\n","        outputs = model(X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch).squeeze()\n","        loss = criterion(outputs, y_batch)\n","        test_losses.append(loss.item())\n","        test_predictions.extend(outputs.tolist())\n","        test_labels.extend(y_batch.tolist())\n","\n","average_test_loss = sum(test_losses) / len(test_losses)\n","print(f'Average test loss: {average_test_loss:.4f}')\n","\n","test_predictions = np.array(test_predictions)\n","test_labels = np.array(test_labels)\n","\n","accuracies(test_labels, test_predictions)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
