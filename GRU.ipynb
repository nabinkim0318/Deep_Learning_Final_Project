{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3900,"status":"ok","timestamp":1699351795827,"user":{"displayName":"The Money","userId":"14405918033498519617"},"user_tz":300},"id":"t1Z4V_Kxw0At"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":756,"status":"ok","timestamp":1699351796576,"user":{"displayName":"The Money","userId":"14405918033498519617"},"user_tz":300},"id":"B06qiDCeB7M4"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n","def accuracies(y_true, y_pred, adaptive=True):\n","    y_true = np.array(y_true).flatten()\n","    y_pred = np.array(y_pred).flatten()\n","    auc = roc_auc_score(y_true, y_pred)\n","\n","    if adaptive == True:\n","        fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n","        dist = fpr ** 2 + (1 - tpr) ** 2\n","        best_thres = thresholds[np.argmin(dist)]\n","    else:\n","        best_thres = 0.5\n","\n","    y_pred_val = np.where(np.array(y_pred).flatten() >= best_thres, 1, 0)\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_val).ravel()\n","    accuracy = (tp + tn) / (tn + fp + fn + tp)\n","    sensitivity = tp / (tp + fn)\n","    specificity = tn / (tn + fp)\n","    print('AUC score:', np.round(auc, 4))\n","    print('Accuracy:', np.round(accuracy, 4))\n","    print('Sensitivity:', np.round(sensitivity, 4))\n","    print('Specificity:', np.round(specificity, 4))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":96019,"status":"error","timestamp":1699351892587,"user":{"displayName":"The Money","userId":"14405918033498519617"},"user_tz":300},"id":"LJl1Y-Ikc7Pm","outputId":"cf683f70-cc38-4c01-fa75-fdfd4a9d4cf2"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 1/1000 [00:08<2:22:37,  8.57s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/1000, Training Loss: 0.5522, Validation Loss: 0.4279\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1000 [00:17<2:22:01,  8.54s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/1000, Training Loss: 0.4419, Validation Loss: 0.4255\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 3/1000 [00:25<2:22:45,  8.59s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/1000, Training Loss: 0.4383, Validation Loss: 0.4233\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 4/1000 [00:33<2:15:46,  8.18s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/1000, Training Loss: 0.4349, Validation Loss: 0.4205\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 5/1000 [00:40<2:07:02,  7.66s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/1000, Training Loss: 0.4290, Validation Loss: 0.4131\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 6/1000 [00:46<2:02:13,  7.38s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/1000, Training Loss: 0.4182, Validation Loss: 0.3964\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 6/1000 [00:53<2:27:24,  8.90s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import numpy as np\n","from tqdm import tqdm\n","from gru import GRUModel\n","\n","torch.manual_seed(0)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","##############################################################\n","###################### Change this part ######################\n","X = np.load('data_X.npy')\n","y = np.load('data_y.npy')\n","\n","#Imputation methods here \n","\n","X = np.nan_to_num(X, nan=1) # You do not have to do this after some traditional imputations\n","##############################################################\n","\n","X_tensor = torch.tensor(X, dtype=torch.float32)\n","y_tensor = torch.tensor(y, dtype=torch.float32)\n","\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","train_index, test_index = next(sss.split(X, y))\n","val_index, test_index = next(StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0).split(X[test_index], y[test_index]))\n","\n","train_dataset = TensorDataset(X_tensor[train_index], y_tensor[train_index])\n","val_dataset = TensorDataset(X_tensor[val_index], y_tensor[val_index])\n","test_dataset = TensorDataset(X_tensor[test_index], y_tensor[test_index])\n","\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","input_size = 5\n","hidden_size = 16\n","output_size = 1\n","lr_rate = 0.001\n","\n","model = GRUModel(input_size, hidden_size, output_size)\n","model.to(device)\n","\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n","\n","early_stopping_patience = 20\n","early_stopping_counter = 0\n","best_val_loss = float('inf')\n","model_path = 'best_model_GRU.pth'\n","\n","num_epochs = 1000\n","for epoch in tqdm(range(num_epochs)):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        predictions = model(inputs).squeeze()\n","        loss = criterion(predictions, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            predictions = model(inputs).squeeze()\n","            loss = criterion(predictions, labels)\n","            val_loss += loss.item() * inputs.size(0)\n","\n","    train_loss = running_loss / len(train_loader.dataset)\n","    val_loss = val_loss / len(val_loader.dataset)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    scheduler.step()\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        early_stopping_counter = 0\n","        torch.save(model.state_dict(), model_path)\n","    else:\n","        early_stopping_counter += 1\n","        if early_stopping_counter >= early_stopping_patience:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","model.load_state_dict(torch.load(model_path), map_location=torch.device('cuda'))\n","\n","model.eval()\n","test_losses = []\n","test_labels = []\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs).squeeze()\n","        loss = criterion(outputs, labels)\n","        test_losses.append(loss.item())\n","        test_predictions.extend(outputs.tolist())\n","        test_labels.extend(labels.tolist())\n","\n","average_test_loss = sum(test_losses) / len(test_losses)\n","print(f'Average test loss: {average_test_loss:.4f}')\n","\n","test_predictions = np.array(test_predictions)\n","test_labels = np.array(test_labels)\n","\n","accuracies(test_labels, test_predictions)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
