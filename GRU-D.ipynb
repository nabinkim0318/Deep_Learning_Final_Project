{"cells":[{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["12744000\n","0.9148972065285624\n"]}],"source":["import numpy as np\n","X = np.load('data_X.npy')\n","y = np.load('data_y.npy')\n","print(X.size)\n","print(np.sum(np.isnan(X))/X.size)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["import torch\n","print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"5J04fUqIvFSS"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/grud.py:114: RuntimeWarning: Mean of empty slice\n","  empirical_mean = np.nanmean(X, axis=1)\n"]},{"name":"stdout","output_type":"stream","text":["True\n","False\n","False\n","False\n","False\n","False\n","True\n","False\n","False\n","False\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 1/1000 [00:58<16:18:48, 58.79s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/1000, Training Loss: 0.5808, Validation Loss: 0.4664\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1000 [01:56<16:03:56, 57.95s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/1000, Training Loss: 0.4513, Validation Loss: 0.4151\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 2/1000 [02:07<17:41:51, 63.84s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-D.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-D.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, y_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-D.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-D.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-D.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ty/Gatech/Fa2023/Deep_Learning_Final_Project/GRU-D.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m X_batch\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/missing-time-series-env/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import numpy as np\n","from tqdm import tqdm\n","import grud\n","import accuracies\n","from datasets import GRUDdataset\n","torch.manual_seed(0)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","X = np.load('data_X.npy')\n","y = np.load('data_y.npy')\n","\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","train_index, test_index = next(sss.split(X, y))\n","val_index, test_index = next(StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0).split(X[test_index], y[test_index]))\n","\n","X, delta, M, last_observation, empirical_mean = grud.preprocess_dataset(X)\n","\n","train_dataset = GRUDdataset(X[train_index], delta[train_index], M[train_index], last_observation[train_index], empirical_mean[train_index], y[train_index])\n","val_dataset = GRUDdataset(X[val_index], delta[val_index], M[val_index], last_observation[val_index], empirical_mean[val_index], y[val_index])\n","test_dataset = GRUDdataset(X[test_index], delta[test_index], M[test_index], last_observation[test_index], empirical_mean[test_index], y[test_index])\n","\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","input_size = 5\n","hidden_size = 16\n","output_size = 1\n","lr_rate = 0.001\n","\n","model = grud.GRU_D(input_size=input_size, hidden_size=hidden_size)\n","model.to(device)\n","\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n","\n","early_stopping_patience = 20\n","early_stopping_counter = 0\n","best_val_loss = float('inf')\n","model_path = 'best_model_GRUD.pth'\n","\n","num_epochs = 1000\n","for epoch in tqdm(range(num_epochs)):\n","    model.train()\n","    running_loss = 0.0\n","    for X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch in train_loader:\n","        X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch \\\n","         = X_batch.to(device), delta_batch.to(device), M_batch.to(device), last_observation_batch.to(device), empirical_mean_batch.to(device), y_batch.to(device)\n","        predictions = model(X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch).squeeze()\n","        loss = criterion(predictions, y_batch)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * X_batch.size(0)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch in val_loader:\n","            X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch \\\n","             = X_batch.to(device), delta_batch.to(device), M_batch.to(device), last_observation_batch.to(device), empirical_mean_batch.to(device), y_batch.to(device)\n","\n","            predictions = model(X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch).squeeze()\n","            loss = criterion(predictions, y_batch)\n","            val_loss += loss.item() * X_batch.size(0)\n","\n","    train_loss = running_loss / len(train_loader.dataset)\n","    val_loss = val_loss / len(val_loader.dataset)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n","\n","    scheduler.step()\n","\n","    if epoch % 10 == 0:\n","        torch.save(model.state_dict(), f'best_model_GRUD_{epoch}.pth')\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        early_stopping_counter = 0\n","        torch.save(model.state_dict(), model_path)\n","    else:\n","        early_stopping_counter += 1\n","        if early_stopping_counter >= early_stopping_patience:\n","            print(\"Early stopping triggered\")\n","            break\n","\n","model.load_state_dict(torch.load(model_path))\n","\n","model.eval()\n","test_losses = []\n","test_labels = []\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch in test_loader:\n","        X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch \\\n","         = X_batch.to(device), delta_batch.to(device), M_batch.to(device), last_observation_batch.to(device), empirical_mean_batch.to(device), y_batch.to(device)\n","        outputs = model(X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch).squeeze()\n","        loss = criterion(outputs, y_batch)\n","        test_losses.append(loss.item())\n","        test_predictions.extend(outputs.tolist())\n","        test_labels.extend(y_batch.tolist())\n","\n","average_test_loss = sum(test_losses) / len(test_losses)\n","print(f'Average test loss: {average_test_loss:.4f}')\n","\n","test_predictions = np.array(test_predictions)\n","test_labels = np.array(test_labels)\n","\n","accuracies.accuracies(test_labels, test_predictions)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"fqRIy9dBOplT"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average test loss: 0.3622\n","AUC score: 0.8319\n","Accuracy: 0.7537\n","Sensitivity: 0.7987\n","Specificity: 0.7438\n"]}],"source":["model.load_state_dict(torch.load(model_path))\n","\n","model.eval()\n","test_losses = []\n","test_labels = []\n","test_predictions = []\n","\n","with torch.no_grad():\n","    for X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch in test_loader:\n","        X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch, y_batch \\\n","         = X_batch.to(device), delta_batch.to(device), M_batch.to(device), last_observation_batch.to(device), empirical_mean_batch.to(device), y_batch.to(device)\n","        outputs = model(X_batch, delta_batch, M_batch, last_observation_batch, empirical_mean_batch).squeeze()\n","        loss = criterion(outputs, y_batch)\n","        test_losses.append(loss.item())\n","        test_predictions.extend(outputs.tolist())\n","        test_labels.extend(y_batch.tolist())\n","\n","average_test_loss = sum(test_losses) / len(test_losses)\n","print(f'Average test loss: {average_test_loss:.4f}')\n","\n","test_predictions = np.array(test_predictions)\n","test_labels = np.array(test_labels)\n","\n","accuracies(test_labels, test_predictions)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
